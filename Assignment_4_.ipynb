{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4 .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4\n",
        "\n",
        "**Boston Housing Dataset**\n",
        "\n",
        "**Predicting Median value of owner-occupied homes **\n",
        "The aim of this assignment is to learn the application of machine learning algorithms to data sets. This involves learning what data means, how to handle data, training, cross validation, prediction, testing your model, etc. This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive, and has been used extensively throughout the literature to benchmark algorithms. The data was originally published by Harrison, D. and Rubinfeld, D.L. Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978. The dataset is small in size with only 506 cases. It can be used to predict the median value of a home, which is done here. There are 14 attributes in each case of the dataset. They are:\n",
        "\n",
        "1. CRIM - per capita crime rate by town\n",
        "2. ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "3. INDUS - proportion of non-retail business acres per town.\n",
        "4. CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
        "5. NOX - nitric oxides concentration (parts per 10 million)\n",
        "6. RM - average number of rooms per dwelling\n",
        "7. AGE - proportion of owner-occupied units built prior to 1940\n",
        "8. DIS - weighted distances to five Boston employment centres\n",
        "9. RAD - index of accessibility to radial highways\n",
        "10. TAX - full-value property-tax rate per $10,000\n",
        "11. PTRATIO - pupil-teacher ratio by town\n",
        "B−1000(Bk−0.63)2  where Bk is the proportion of blacks by town\n",
        "12. LSTAT - % lower status of the population\n",
        "13. MEDV - Median value of owner-occupied homes in $1000's"
      ],
      "metadata": {
        "id": "jx4Ei6-Y5dxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aim\n",
        "**bold text**\n",
        "-To implement a linear regression with regularization via gradient descent.\n",
        "\n",
        "-To implement gradient descent with Lp norm, for 3 different values of p in (1,2]\n",
        "\n",
        "-To contrast the difference between performance of linear regression Lp norm and L2 norm for these 3 different values.\n",
        "\n",
        "-Tally that the gradient descent for L2 gives same result as matrix inversion based solution.\n",
        "\n",
        "-All the code is written in a single python file. The python program accepts the data directory path as input where the train and test csv files reside. -Note that the data directory will contain two files train.csv used to train\n",
        " your model and test.csv for which the output predictions are to be made. \n",
        " -The output predictions get written to a file named output.csv. The output.csv file should have two comma separated columns [ID,Output].\n",
        "\n",
        "\n",
        "**Working of Code**\n",
        "\n",
        "-NumPy library would be required, so code begins by importing it.\n",
        "\n",
        "-Import phi and phi_test from train and test datasets using NumPy's loadtxt function\n",
        "\n",
        "-Import y from train dataset using the loadtxt function\n",
        "\n",
        "-Concatenate coloumn of 1s to right of phi and phi_test\n",
        "\n",
        "-Apply min max scaling on each coloumn of phi and phi_test\n",
        "\n",
        "-Apply log scaling on y\n",
        "\n",
        "-Define a function to calculate change in error function based on phi, w and p norm\n",
        "\n",
        "-Make a dictionary containing filenames as keys and p as values\n",
        "\n",
        "-For each item in this dictionary\n",
        "\n",
        "-Set the w to all 0s\n",
        "\n",
        "-Set an appropriate value for lambda and step size\n",
        "\n",
        "-Calculate new value of w\n",
        "\n",
        "Repeat steps until error between consecutive ws is less than threshold\n",
        "\n",
        "-Load values of id from test data file\n",
        "\n",
        "-Calculate y for test data using phi test and applying inverse log\n",
        "\n",
        "-Save the ids and y according to filename from dictionary\n",
        "\n",
        "-Feature Engineering\n",
        "\n",
        "-Columns of phi are not in same range, this is because their units are different i.e phi is ill conditioned\n",
        "\n",
        "So, min max scaling for each column is applied to bring them in range 0-1\n",
        "\n",
        "-Same scaling would be required on columns of phi test\n",
        "\n",
        "-Log scaling was used on y. This was determined by trial and error\n",
        "\n",
        "-Comparison of performance\n",
        "(p1=1.75, p2=1.5, p3=1.3)\n",
        "\n",
        "-As p decreases error in y decreases\n",
        "\n",
        "-As p decreases norm of w increases but this can be taken care by increasing lambda\n",
        "\n",
        "-As p decreases number of iterations required decreases\n",
        "\n",
        "-Tuning of Hyperparameter\n",
        "\n",
        "-If p is fixed and lambda is increased error decreases up to a certain lambda, then it starts rising\n",
        "\n",
        "\n",
        "-So, lambda was tuned by trial and error.-Starting with 0, lambda was increased in small steps until a minimum error was achieved.\n",
        "\n",
        "-Comparison of L2 gradient descent and closed form\n",
        "\n",
        "-Error from L2 Gradient descent were 4.43268 and that \n",
        "from closed form solution was 4.52624.\n",
        "\n",
        "-Errors are comparable so, the L2 gradient descent performs closely with closed form solution.\n"
      ],
      "metadata": {
        "id": "e7lizKWA-MYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boston Housing Dataset"
      ],
      "metadata": {
        "id": "xaREs4BZBIes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "oXhlBskg5cnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First import train dataset**"
      ],
      "metadata": {
        "id": "W6q6tS7G5cYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DthabLh25cU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/train.csv\")"
      ],
      "metadata": {
        "id": "WZInDvtm5cDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "V8O3u8u15b_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "dadHP-OW5b9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "giIuGBpg5bjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset doesn't have any null values."
      ],
      "metadata": {
        "id": "oLE70g7r5bVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "UqB_i76z5auW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deleting Duplicates"
      ],
      "metadata": {
        "id": "PaprFATS5aYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "d8-nlM4s5aFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "XxitVWMT5Z6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the shape it's clear that we have no duplicates.**"
      ],
      "metadata": {
        "id": "WgJ576ra5ZuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding**"
      ],
      "metadata": {
        "id": "L2h3N4aj5ZSZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS0DvuhE2OI0"
      },
      "outputs": [],
      "source": [
        "b = []\n",
        "for i in df.keys():\n",
        "  b.append(i)\n",
        "print(b) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, columns = ['CHAS'])"
      ],
      "metadata": {
        "id": "zTh_x4ATDdKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "kzXw_-EsDeIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking if ouliers exist or not and EDA** "
      ],
      "metadata": {
        "id": "DeH3UepODlOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.describe().T\n",
        "x"
      ],
      "metadata": {
        "id": "LvySFWGQDwZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outlierpresence(df):\n",
        "  for i in df.keys():\n",
        "    Q1 = df.quantile(0.25)\n",
        "    Q3 = df.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    x = (df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))\n",
        "    # df[x.isin([True])]\n",
        "    substring = 'True'\n",
        "    y= x[x.apply(lambda row: row.astype(str).str.contains(substring, case=False).any(), axis=1)] #IT WILL GIVE ALL OUTLIERS IN THE DATAFRAME WITH ALL COLUMNS\n",
        "    if True in y[i].tolist(): #HERE WE CHECK True is in the list of particular column\n",
        "      print('Outliers', '\\033[1m'+ 'present' +'\\033[0m', 'in the data of','\\033[1m' + i + '\\033[0m')\n",
        "      print('-------------------------------')\n",
        "    else:\n",
        "      print('Outliers', '\\033[1m'+ ' not present in the data of' +'\\033[0m', 'in','\\033[1m' + i + '\\033[0m') \n",
        "      print('-------------------------------') \n",
        "outlierpresence(df)"
      ],
      "metadata": {
        "id": "7c3idRLmDyoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loweruppwhisker(df):\n",
        "  for i in df.keys():\n",
        "    Q1 = df[i].quantile(0.25)\n",
        "    Q3 = df[i].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    whisker_width = 1.5\n",
        "    lower_whisker = Q1 -(whisker_width*IQR)\n",
        "    upper_whisker = Q3 + (whisker_width*IQR)\n",
        "    print('\\033[1m' + i + '\\033[0m')\n",
        "    print('-------------------------')\n",
        "    print(\"Lowe whisker: \",lower_whisker)\n",
        "    print(\"Upper whisker: \", upper_whisker)\n",
        "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
        "loweruppwhisker(df)"
      ],
      "metadata": {
        "id": "mSIAOpZCD0rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in df.items():\n",
        "  q1 = v.quantile(0.25)\n",
        "  q3 = v.quantile(0.75)\n",
        "  irq = q3 - q1\n",
        "  v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n",
        "  perc = np.shape(v_col)[0] * 100.0 / np.shape(df)[0]\n",
        "  print(\"Column %s outliers = %.2f%%\" % (k, perc))"
      ],
      "metadata": {
        "id": "LxoAgucZD3BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (16, 12))\n",
        "sns.heatmap(df.corr(), annot = True, fmt = '.2%')\n",
        "# plt.savefig('../images/features_correlation.png')"
      ],
      "metadata": {
        "id": "SSrBglYyD5HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme()"
      ],
      "metadata": {
        "id": "Wz78CBEKD7Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the correlation plot we can see greater POSITIVE correlation in this order  RM>ZN>B>DIS>CHS_1**"
      ],
      "metadata": {
        "id": "ceq4G1xmD9t1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Price of home vs average homes per dwelling')\n",
        "sns.scatterplot(data=df, x=df['MEDV'], y=df['RM'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r8qocm8oEESe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['MEDV','RM']].corr()"
      ],
      "metadata": {
        "id": "0pFZLXj2EF0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Good linear relationship and correlation**"
      ],
      "metadata": {
        "id": "qwB7MIaNEIk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For other main factors **"
      ],
      "metadata": {
        "id": "CJhvAIBeEWVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Price of home vs residential land zoned for lots over 25,000 sq.ft')\n",
        "sns.scatterplot(data=df, x=df['MEDV'], y=df['ZN'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EAEcilXSEH9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['MEDV','ZN']].corr()"
      ],
      "metadata": {
        "id": "nO60dQ6UEjEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Price of home VS proportion of blacks by town')\n",
        "sns.scatterplot(data=df, x=df['MEDV'], y=df['B'])\n",
        "plt.show()\n",
        "df[['MEDV','B']].corr()"
      ],
      "metadata": {
        "id": "fJBVc6ydEkhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Price of home VS weighted distances to five Boston employment centres')\n",
        "sns.scatterplot(data=df, x=df['MEDV'], y=df['DIS'])\n",
        "plt.show()\n",
        "df[['MEDV','DIS']].corr()"
      ],
      "metadata": {
        "id": "0I_m_wW6EmRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph of negative corelation **"
      ],
      "metadata": {
        "id": "g4aNwWdvEpLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Price of home per capita crime rate by town')\n",
        "sns.scatterplot(data=df, x=df['MEDV'], y=df['CRIM'])\n",
        "plt.show()\n",
        "df[['MEDV','CRIM']].corr()"
      ],
      "metadata": {
        "id": "QEc34BCQEvxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Price of home VS proportion of non-retail business acres per town.')\n",
        "sns.scatterplot(data=df, x=df['MEDV'], y=df['INDUS'])\n",
        "plt.show()\n",
        "df[['MEDV','INDUS']].corr()"
      ],
      "metadata": {
        "id": "K3O7lAUxEyGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Price of home VS nitric oxides concentration')\n",
        "sns.scatterplot(data=df, x=df['MEDV'], y=df['NOX'])\n",
        "plt.show()\n",
        "df[['MEDV','NOX']].corr()"
      ],
      "metadata": {
        "id": "dpLmlNBzEz5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Price of home VS proportion of owner-occupied units built prior to 1940')\n",
        "sns.scatterplot(data=df, x=df['MEDV'], y=df['AGE'])\n",
        "plt.show()\n",
        "df[['MEDV','AGE']].corr()"
      ],
      "metadata": {
        "id": "Q3GGPUCxE4od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Price of home VS index of accessibility to radial highways')\n",
        "sns.scatterplot(data=df, x=df['MEDV'], y=df['RAD'])\n",
        "plt.show()\n",
        "df[['MEDV','RAD']].corr()"
      ],
      "metadata": {
        "id": "xZ_R5FuOFAPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Price of home VS full-value property-tax rate per \\$10,000')\n",
        "sns.scatterplot(data=df, x=df['MEDV'], y=df['TAX'])\n",
        "plt.show()\n",
        "df[['MEDV','TAX']].corr()"
      ],
      "metadata": {
        "id": "9kG4xWEFFCD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Price of home VS pupil-teacher ratio by town')\n",
        "sns.scatterplot(data=df, x=df['MEDV'], y=df['PTRATIO'])\n",
        "plt.show()\n",
        "df[['MEDV','PTRATIO']].corr()"
      ],
      "metadata": {
        "id": "ntjhojIKFDvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPLITTING**"
      ],
      "metadata": {
        "id": "Rb0rJlHuFGX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = []\n",
        "for i in df.keys():\n",
        "  b.append(i)\n",
        "print(b) "
      ],
      "metadata": {
        "id": "iyLFP-qgFFkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No need for 'MEDV' and 'ID' in the feature b"
      ],
      "metadata": {
        "id": "rwP6ICH4FSjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b.remove('ID')\n",
        "b.remove('MEDV')\n",
        "print(b)"
      ],
      "metadata": {
        "id": "6xdlwG-7FgPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[b].values#array of features\n",
        "y = df['MEDV'].values"
      ],
      "metadata": {
        "id": "0OgyZOxtFj6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "wdfvw6xRFnqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler ## standrard scalig \n",
        "scaler = StandardScaler() #initialise to a variable\n",
        "scaler.fit(X_train,y_train) # we are finding the values of mean and sd from the td\n",
        "X_train_scaled = scaler.transform(X_train) # fit (mean, sd) and then transform the training data\n",
        "X_test_scaled = scaler.transform(X_test) # transform the test data "
      ],
      "metadata": {
        "id": "Q4MT5ndyFplK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**"
      ],
      "metadata": {
        "id": "dYw6z8vFFuHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "H-MqrcmcF1Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coeff_df = pd.DataFrame(regressor.coef_,[b], columns=['Coefficient'])\n",
        "y_pred = regressor.predict(X_test_scaled)\n",
        "coeff_df"
      ],
      "metadata": {
        "id": "vtMRK3JUF4Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)"
      ],
      "metadata": {
        "id": "kGAvmf5fF801"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.intercept_ # c "
      ],
      "metadata": {
        "id": "4yUfleXJF-bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "df"
      ],
      "metadata": {
        "id": "ff_8ZG73GAPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print('R2- SCORE:', metrics.r2_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "6vAsiz5BGCb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we have 73% accuracy of R-2 score**"
      ],
      "metadata": {
        "id": "U2AN4BA6Ft6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Selection**\n",
        "\n",
        "It will show the most important feature with ranking."
      ],
      "metadata": {
        "id": "wGxTdD9mGOeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "estimator = LinearRegression()\n",
        "selector = RFE(estimator, n_features_to_select=3, step=1)\n",
        "selector = selector.fit(X_train_scaled, y_train)\n",
        "sorted(list(zip(selector.ranking_,b)))"
      ],
      "metadata": {
        "id": "7Nsgu078GVpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UPLOAD TEST DATASET"
      ],
      "metadata": {
        "id": "9lyJwM2kGaL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"\")"
      ],
      "metadata": {
        "id": "ro6Y1S28GzBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "R_CvM0IqG1PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have no 'MEDV' column so we want to predict that value from our trained model"
      ],
      "metadata": {
        "id": "_Yyk4S6uG4fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly encode 'CHAS' column for the new test.csv file"
      ],
      "metadata": {
        "id": "DKJE6r0XG8ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.get_dummies(df1, columns = ['CHAS'])"
      ],
      "metadata": {
        "id": "-u5GZ32_G26E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = []\n",
        "for i in df1.keys():\n",
        "  c.append(i)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "Z6-LvR3wHCcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c.remove('ID')"
      ],
      "metadata": {
        "id": "RaWyyfFgHEHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now take the feature value as the given data**"
      ],
      "metadata": {
        "id": "mWnmw7L3HJYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1[c].values"
      ],
      "metadata": {
        "id": "ZBfcLHm9HNnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "h7cwN7uAHO1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_scale = scaler.transform(X) #scale the data of features X"
      ],
      "metadata": {
        "id": "PmrVnCE_HQor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the next step with our trained regressor, we predict new values of target variable 'MEDV'**"
      ],
      "metadata": {
        "id": "FrJ70tzVHSlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_testpred = regressor.predict(X_test_scale) "
      ],
      "metadata": {
        "id": "u0viQ8ArHbGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_testpred)"
      ],
      "metadata": {
        "id": "xsD20rQ-HcaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pridicted value is addesd to our new dataframe"
      ],
      "metadata": {
        "id": "IPKcy7hYHe3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1['MEDV'] = y_testpred"
      ],
      "metadata": {
        "id": "yMSzKIDJHeMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "98QG_WGnHnko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save new csv file as output.csv"
      ],
      "metadata": {
        "id": "6qR5ZLfaHpjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.to_csv('output.csv', index=False)"
      ],
      "metadata": {
        "id": "vLQ2Dx77HwkB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}